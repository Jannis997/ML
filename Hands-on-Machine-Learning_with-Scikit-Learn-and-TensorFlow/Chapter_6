1. What is the approximate depth of a Decision Tree trained (without restrictions)
on a training set with 1 million instances?
2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐
ally lower/greater, or always lower/greater?
3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing
max_depth?
4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling
the input features?
5. If it takes one hour to train a Decision Tree on a training set containing 1 million
instances, roughly how much time will it take to train another Decision Tree on a
training set containing 10 million instances?
6. If your training set contains 100,000 instances, will setting presort=True speed
up training?


1. log2(m)
2. Generally lower but not always
3. Yes, since it applies regularization
4. Scaling has no effect on the decision tree so no
5. Complexity for training: O(n × m log(m))
  For the factor the instances m has to be multiplied by 10 and the whole has to be devided
  by the original complexity:
  K = (n × 10m × log(10m)) / (n × m × log(m)) = 10 × log(10m) / log(m)
  for m=1,000,000 the result is 11.7 times training time of 1 hour
6. Presorting speeds up training if there are only a few thousand instances, for 100,000 instances
  the trainings time would increase


