1. What Linear Regression training algorithm can you use if you have a training set
with millions of features?
2. Suppose the features in your training set have very different scales. What algo‐
rithms might suffer from this, and how? What can you do about it?
3. Can Gradient Descent get stuck in a local minimum when training a Logistic
Regression model?
4. Do all Gradient Descent algorithms lead to the same model provided you let
them run long enough?
5. Suppose you use Batch Gradient Descent and you plot the validation error at
every epoch. If you notice that the validation error consistently goes up, what is
likely going on? How can you fix this?
6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐
dation error goes up?
7. Which Gradient Descent algorithm (among those we discussed) will reach the
vicinity of the optimal solution the fastest? Which will actually converge? How
can you make the others converge as well?
8. Suppose you are using Polynomial Regression. You plot the learning curves and
you notice that there is a large gap between the training error and the validation
error. What is happening? What are three ways to solve this?
9. Suppose you are using Ridge Regression and you notice that the training error
and the validation error are almost equal and fairly high. Would you say that the
model suffers from high bias or high variance? Should you increase the regulari‐
zation hyperparameter α or reduce it?
10. Why would you want to use:
• Ridge Regression instead of Linear Regression?
• Lasso instead of Ridge Regression?
• Elastic Net instead of Lasso?
11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.
Should you implement two Logistic Regression classifiers or one Softmax Regres‐
sion classifier?
12. Implement Batch Gradient Descent with early stopping for Softmax Regression
(without using Scikit-Learn).

1. Batch GD, Stochastic GD, Mini-batch GD
2. All from 1. would suffer because the shape function would be have the forma of 
  an ellipse(in 2D) and not a circle, this would lead to the GD not going straight
  to the minimum but go in a curve. You can mormalize the features
3. No
4. No, Stochastic GD and Mini-batchGD will never settle at the minimum except you
  gradually decrease the learning rate
5. The model overfitted the data and surpassed its optimal point
6. No, for Mini-batch GD, the validation error is not continuos, it could be that the 
  model only had a set back but will reach a lower validation error
7. The fastest is dependend on n and m ( But Stochastic GD in general). Normal Equation and Batch GD will converge. 
  For the other two the learning rate has to gradually decrease to converge
8. The model overfitted, hence the error for the trainings data is small but big for 
  the validation data. Three ways to solve the issue:
  • use lower polynomial
  • use regularization method
  • use early stopping
9. High bias, Decrease it so the complexity of the model increases (its underfitted)
10. 
  • A bit regularization is almost always helpful so ridge regression over linear
  • If you suspect that many features are not useful
  • Elastic Net if the number of features are higher than number of instances, 
  but also in general elastic
11. two logistic regression since softmax regression can only select 1 and cant
  handle problems where 2 solutions are possible
12. 
